{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipy_pdcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import tempfile\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pybiomart\n",
    "from gene_map import GeneMapper\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import load_config, split_df_row\n",
    "from tad_helper_functions import parse_tad_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm_orig\n",
    "tqdm_orig.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "cache_dir = config['output_dirs']['cache']\n",
    "images_dir = config['output_dirs']['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet = pd.read_table(\n",
    "    config['input_files']['raw_disgenet'],\n",
    "    usecols=['snpId','diseaseId','diseaseName','source'])\n",
    "df_disgenet['diseaseIdType'] = 'UMLS_CUI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "tad_data_fname = f'{results_dir}/tads_hg38.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate latest GWAS-catalog version\n",
    "\n",
    "Column definitions: https://www.ebi.ac.uk/gwas/docs/methods/curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gwascat = pd.read_table(config['input_files']['raw_gwascatalog'], low_memory=False)\n",
    "\n",
    "#df_gwascat = df_gwascat[['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT']]\n",
    "df_gwascat.dropna(subset=['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT'], inplace=True)\n",
    "df_gwascat.rename(columns={\n",
    "    'SNP_ID_CURRENT': 'snpId', 'MAPPED_TRAIT_URI': 'diseaseId', 'MAPPED_TRAIT': 'diseaseName'\n",
    "}, inplace=True)\n",
    "\n",
    "df_gwascat['snpId'] = df_gwascat['snpId'].apply(lambda x: f'rs{x}')\n",
    "df_gwascat['source'] = 'GWASCUSTOM'\n",
    "df_gwascat = split_df_row(df_gwascat, 'diseaseId', ',')\n",
    "df_gwascat['diseaseId'] = df_gwascat['diseaseId'].apply(lambda x: x.split('/')[-1])\n",
    "df_gwascat['diseaseIdType'] = df_gwascat['diseaseId'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# convert BETA to odds ratio\n",
    "df_gwascat['odds_ratio'] = df_gwascat['OR or BETA'].apply(lambda x: np.exp(x) if x < 1 else x)\n",
    "\n",
    "df_gwascat.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_gwascat])  #df_disgenet, \n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer associated gene(s)\n",
    "\n",
    "Possible columns:\n",
    "* REPORTED GENE(S): gene reported by author\n",
    "* MAPPED GENE: Gene(s) mapped to the strongest SNP (if SNP is intergenic uses upstream and downstream genes)\n",
    "* SNP_GENE_IDS: Entrez Gene ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['REPORTED GENE(S)', 'MAPPED_GENE', 'SNP_GENE_IDS']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_source = config['parameters']['associated_gene_source']\n",
    "\n",
    "if gene_source == 'reported':\n",
    "    # are gene names, must be mapped to ENTREZ\n",
    "    raw_genes = df['REPORTED GENE(S)'].str.split(', ').tolist()\n",
    "    \n",
    "    gene_blacklist = {'intergenic', 'NR'}\n",
    "    cur_genes = [g for gs in raw_genes if not isinstance(gs, float) for g in gs if g not in gene_blacklist]  # isinstance(gs,float) -> gs==np.nan\n",
    "    \n",
    "    gm = GeneMapper()\n",
    "    df_map = gm.query(id_list=cur_genes, source_id_type='Gene_Name', target_id_type='GeneID')\n",
    "    name2id = df_map.set_index('ID_from').to_dict()['ID_to']\n",
    "    \n",
    "    entrez_genes = [None \n",
    "                    if isinstance(gs, float) \n",
    "                    else [name2id[g] for g in gs if g in name2id]\n",
    "                    for gs in raw_genes]\n",
    "elif gene_source == 'mapped':\n",
    "    # are already ENTREZ IDs\n",
    "    raw_genes = df['SNP_GENE_IDS'].str.split(', ').tolist()\n",
    "    entrez_genes = [None if isinstance(gs, float) else gs for gs in raw_genes]\n",
    "else:\n",
    "    raise RuntimeError(f'Invalid gene source: \"{gene_source}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['associated_genes'] = [None if gs is None else ','.join(gs) for gs in entrez_genes]\n",
    "df[['REPORTED GENE(S)', 'MAPPED_GENE', 'SNP_GENE_IDS', 'associated_genes']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Ontology OWLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_owl_file(soup, relevant_terms):\n",
    "    \"\"\" Extract requested terms from OWL-file\n",
    "    \"\"\"\n",
    "    node_owl_data = {}\n",
    "    for entry in tqdm(soup.find_all('Class')):\n",
    "        doid = entry['rdf:about'].split('/')[-1]\n",
    "\n",
    "        # get label\n",
    "        lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "        # get requested terms\n",
    "        term_map = {term: [] for term in relevant_terms}\n",
    "        for xref in entry.find_all('oboInOwl:hasDbXref'):\n",
    "            txt = xref.get_text()\n",
    "            for term in relevant_terms:\n",
    "                if txt.startswith(f'{term}:'):\n",
    "                    idx = txt.split(':')[-1]\n",
    "                    term_map[term].append(idx)\n",
    "\n",
    "        assert doid not in node_owl_data, doid\n",
    "        node_owl_data[doid] = {\n",
    "            'label': lbl,\n",
    "            'terms': term_map\n",
    "        }\n",
    "        \n",
    "    return node_owl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['input_files']['disease_ontology']) as fd:\n",
    "    soup_doid = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_owl_data_doid = parse_owl_file(soup_doid, ['UMLS_CUI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save EFO disease labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['input_files']['exp_factor_ontology']) as fd:\n",
    "    soup = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_lbl_map = {}\n",
    "for entry in tqdm(soup.find_all('Class')):\n",
    "    if not entry.has_attr('rdf:about'):\n",
    "        continue\n",
    "    \n",
    "    efo = entry['rdf:about'].split('/')[-1]\n",
    "    lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "    assert efo not in efo_lbl_map\n",
    "    efo_lbl_map[efo] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "\n",
    "df_efolabels = pd.DataFrame(list(efo_lbl_map.items()), columns=['EFO', 'label'])\n",
    "df_efolabels.to_csv(f'{results_dir}/disease_efolabels.csv', index=False)\n",
    "\n",
    "print(df_efolabels.shape)\n",
    "df_efolabels.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease ontology as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontology_network(fname_in):\n",
    "    type_ = os.path.basename(fname_in).split('.')[0]\n",
    "    #assert type_ in ('efo', 'doid'), f'Invalid type: {type_}'\n",
    "    \n",
    "    fname_out = f'{cache_dir}/{type_}_graph.edgelist.gz'\n",
    "    if not os.path.exists(fname_out):\n",
    "        import onto2nx  # https://github.com/cthoyt/onto2nx\n",
    "        nx.write_edgelist(onto2nx.parse_owl_rdf(fname_in), fname_out)\n",
    "    else:\n",
    "        print('Cached', fname_out)\n",
    "        \n",
    "    graph = nx.read_edgelist(fname_out, create_using=nx.DiGraph()).reverse()\n",
    "    graph.name = type_\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_graph = load_ontology_network(config['input_files']['disease_ontology'])\n",
    "print(nx.info(doid_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_graph = load_ontology_network(config['input_files']['exp_factor_ontology'])\n",
    "print(nx.info(efo_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map UMLS_CUI to DOID node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_umls_map = {}\n",
    "for node, data in tqdm(node_owl_data_doid.items()):\n",
    "    assert set(data['terms'].keys()) == set(['UMLS_CUI'])\n",
    "    doid_umls_map[node] = data['terms']['UMLS_CUI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find cancer subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_diseases = set(df.diseaseId.unique())\n",
    "\n",
    "# gather all possible disease-nodes\n",
    "all_nodes_umls = [umls for doid in doid_graph.nodes() for umls in doid_umls_map[doid]]\n",
    "all_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000408')) + ['EFO_0000408']  # disease subtree (vs traits, ...)\n",
    "\n",
    "all_nodes = set(all_nodes_efo + all_nodes_umls) & given_diseases\n",
    "\n",
    "# extract all cancer diseases\n",
    "cancer_nodes_doid = (list(nx.descendants(doid_graph, 'DOID_162')) + ['DOID_162'])  # cancer subtree\n",
    "cancer_nodes_umls = [umls for doid in cancer_nodes_doid for umls in doid_umls_map[doid]]\n",
    "\n",
    "cancer_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000311')) + ['EFO_0000311']  # cancer subtree\n",
    "\n",
    "cancer_nodes = set(cancer_nodes_efo + cancer_nodes_umls) & given_diseases\n",
    "assert cancer_nodes <= all_nodes\n",
    "print(f'#various nodes: {len(cancer_nodes)}/{len(all_nodes)}/{len(given_diseases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do cancer-classification\n",
    "data_cancer = []\n",
    "for disease in tqdm(df['diseaseId'].unique()):\n",
    "    if disease in all_nodes:\n",
    "        data_cancer.append((disease, disease in cancer_nodes))\n",
    "    \n",
    "df_iscancer = pd.DataFrame(data_cancer, columns=['diseaseId','is_cancer'])\n",
    "df_iscancer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_iscancer.to_csv(f'{results_dir}/disease_cancer_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve VEP annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant consequence ontology: http://www.sequenceontology.org/browser/current_release\n",
    "\n",
    "Description of variant types: https://www.ensembl.org/info/genome/variation/prediction/predicted_data.html\n",
    "\n",
    "Raw data: ftp://ftp.ensembl.org/pub/release-98/variation/vep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = df['snpId'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_version = config['parameters']['source_genomiccoordinates_version']\n",
    "\n",
    "if coord_version == 'hg38':\n",
    "    mart_host = 'www.ensembl.org'\n",
    "elif coord_version == 'hg19':\n",
    "    mart_host = 'grch37.ensembl.org'\n",
    "else:\n",
    "    raise RuntimeError(f'Invalid coordinate version: \"{coord_version}\"')\n",
    "    \n",
    "print(f'Using {mart_host}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%pdcache df_anno_raw $cache_dir/snp_annotations_raw.csv\n",
    "\n",
    "# retrieve SNP annotations\n",
    "df_anno_raw = None\n",
    "while df_anno_raw is None:\n",
    "    try:\n",
    "        server = pybiomart.Server(host=mart_host)\n",
    "        dataset = server.marts['ENSEMBL_MART_SNP'].datasets['hsapiens_snp']\n",
    "\n",
    "        df_anno_raw = dataset.query(\n",
    "            attributes=['refsnp_id', 'chr_name', 'chrom_start', 'consequence_type_tv', 'ensembl_transcript_stable_id'],\n",
    "            filters={'snp_filter': snps},\n",
    "            use_attr_names=True)\n",
    "    except requests.HTTPError:\n",
    "        # retry if network error occurred\n",
    "        print('Next try...')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert annotations to usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = df_anno_raw.copy()\n",
    "\n",
    "# processing preparations\n",
    "df_anno['chr_name'] = df_anno['chr_name'].astype(str)\n",
    "\n",
    "# remove haplotypes (e.g. CHR_HSCHR6_MHC_COX_CTG1)\n",
    "df_anno = df_anno[~df_anno['chr_name'].str.contains('_')]\n",
    "\n",
    "# mark empty consequence as 'intergenic' (NaN in dataframe shows up as intergenic in VEP web-interface)\n",
    "df_anno.loc[df_anno['consequence_type_tv'].isna(), 'consequence_type_tv'] = 'intergenic_variant'\n",
    "\n",
    "# select most frequent annotations (TODO: handle multiple maxima)\n",
    "tmp  = []\n",
    "for snp, group in tqdm(df_anno.groupby('refsnp_id')):\n",
    "    top_vep = group['consequence_type_tv'].value_counts().idxmax()\n",
    "    match = group[group['consequence_type_tv'] == top_vep].iloc[0]\n",
    "    tmp.append(match)\n",
    "df_anno = pd.DataFrame(tmp)\n",
    "\n",
    "# set column names\n",
    "df_anno.drop('ensembl_transcript_stable_id', axis=1, inplace=True)\n",
    "\n",
    "df_anno.rename(\n",
    "    columns={\n",
    "        'refsnp_id': 'snpId', 'consequence_type_tv': 'variant_type',\n",
    "        'chr_name': 'chromosome', 'chrom_start': 'position'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group variant types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sequence ontology (SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_so = load_ontology_network(config['input_files']['sequence_ontology'])\n",
    "print(nx.info(graph_so))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_subgraph = list(nx.descendants(graph_so, 'SO_0001791')) + ['SO_0001791']\n",
    "intron_subgraph = list(nx.descendants(graph_so, 'SO_0001627')) + ['SO_0001627']\n",
    "intergenic_subgraph = list(nx.descendants(graph_so, 'SO_0001628')) + ['SO_0001628']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find ontology labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['input_files']['sequence_ontology']) as fd:\n",
    "    soup_so = BeautifulSoup(fd, 'xml')\n",
    "\n",
    "so_label_map = {}\n",
    "for entry in tqdm(soup_so.find_all('Class')):\n",
    "    if 'rdf:about' not in entry.attrs:\n",
    "        continue\n",
    "    \n",
    "    idx = entry['rdf:about'].split('/')[-1]\n",
    "    if not idx.startswith('SO_'):\n",
    "        continue\n",
    "\n",
    "    label_entry = entry.find('rdfs:label')\n",
    "    if label_entry is None:\n",
    "        # probably deprecated\n",
    "        continue\n",
    "    lbl = label_entry.get_text()\n",
    "\n",
    "    so_label_map[lbl] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_vep(vep):\n",
    "    special_cases = {\n",
    "        'NMD_transcript_variant': 'exonic',\n",
    "        'mature_miRNA_variant': 'exonic'\n",
    "    }\n",
    "    \n",
    "    if so_label_map[vep] in exon_subgraph:\n",
    "        return 'exonic'\n",
    "    elif so_label_map[vep] in intron_subgraph:\n",
    "        return 'intronic'\n",
    "    elif so_label_map[vep] in intergenic_subgraph:\n",
    "        return 'intergenic'\n",
    "    else:\n",
    "        return special_cases.get(vep, 'ambiguous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno['variant_group'] = df_anno['variant_type'].apply(classify_vep)\n",
    "df_anno['variant_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that all SNPs have been annotated (TODO: make this rigorous)\n",
    "#assert set(df_anno['snpId'].tolist()) == set(snps), set(snps) - set(df_anno['snpId'].tolist())\n",
    "assert df_anno is not None\n",
    "assert df_anno.shape[0] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that all variant types have been grouped\n",
    "assert df_anno['variant_group'].isna().sum() == 0, df_anno[df_anno.variant_group.isna()].drop_duplicates('variant_type')['variant_type'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that variant type groups are reasonable\n",
    "assert set(df_anno['variant_group']) <= {'exonic', 'intronic', 'intergenic', 'ambiguous'}, df_anno['variant_group'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "print('#SNPs in database:', df['snpId'].unique().size, f'({len(snps)})')\n",
    "print('#annotated SNPs:', df_anno['snpId'].size)\n",
    "print('#intersection:', len(set(df['snpId'].tolist()) & set(df_anno['snpId'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer TAD relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SNP positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snppos = df_anno[['snpId', 'chromosome', 'position']].copy()\n",
    "df_snppos.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads = pd.read_table(tad_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAD statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_lengths = df_tads['tad_stop'] - df_tads['tad_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_, max_ = tad_lengths.min(), tad_lengths.max()\n",
    "min_ = min_ if min_ > 0 else 1\n",
    "if min_ == max_:\n",
    "    min_ -= 10\n",
    "    max_ += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10**np.linspace(np.log10(min_), np.log10(max_), 50)\n",
    "sns.distplot(tad_lengths, kde=False, bins=bins)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('TAD length')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(images_dir, 'tad_length_histogram.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_range_dict(row, dict_):\n",
    "    range_dict_ = dict_.get(str(row['chromosome']), None)\n",
    "    if range_dict_ is None:\n",
    "        return 'undef'\n",
    "    \n",
    "    try:\n",
    "        return range_dict_[row['position']]\n",
    "    except KeyError:\n",
    "        return 'outside'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_anno_20in = parse_tad_annotations('20in', fname=tad_data_fname)\n",
    "df_snppos['TAD_20in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20in), axis=1)\n",
    "\n",
    "tad_anno_40in = parse_tad_annotations('40in', fname=tad_data_fname)\n",
    "df_snppos['TAD_40in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40in), axis=1)\n",
    "\n",
    "tad_anno_20out = parse_tad_annotations('20out', fname=tad_data_fname)\n",
    "df_snppos['TAD_20out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20out), axis=1)\n",
    "\n",
    "tad_anno_40out = parse_tad_annotations('40out', fname=tad_data_fname)\n",
    "df_snppos['TAD_40out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40out), axis=1)\n",
    "\n",
    "tad_anno_20inout = parse_tad_annotations('20inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_20inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20inout), axis=1)\n",
    "\n",
    "tad_anno_40inout = parse_tad_annotations('40inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_40inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40inout), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snptads = df_snppos.drop(['chromosome', 'position'], axis=1)\n",
    "df_snptads.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible cell values:\n",
    "* `tad`: SNP is in TAD body (i.e. not in boundary)\n",
    "* `boundary`: SNP is in TAD boundary\n",
    "* `undef`: chromosome that SNP is in has no TAD information available\n",
    "* `outside`: SNP is outside of TAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge into DisGeNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual databases\n",
    "print('DisGeNET only:', df_disgenet.shape)\n",
    "print('Most recent GWAS-catalog: ', df_gwascat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial aggregation\n",
    "df_final = df.copy()\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer-classification\n",
    "df_final = df_final.merge(df_iscancer, on='diseaseId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAD localization\n",
    "df_final = df_final.merge(df_snptads, on='snpId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNP annotation\n",
    "df_final = df_final.merge(df_anno, how='left')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pre-filtering:', df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['filters']['OR_threshold'] is not None:\n",
    "    df_final = df_final[df_final['odds_ratio'] > config['filters']['OR_threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant type (group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['filters']['variant_type_group'] is not None:\n",
    "    df_final = df_final[df_final['variant_group'] == config['filters']['variant_type_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_final.to_csv(f'{results_dir}/snpdb_enhanced.tsv', sep='\\t', index=False)\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot database statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of entries per disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_counts = (df_final['diseaseId']\n",
    "                  .value_counts()\n",
    "                  .rename('count')\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'index': 'diseaseId'})\n",
    "                  .sort_values('count')\n",
    "                  .merge(df_iscancer, on='diseaseId'))\n",
    "\n",
    "disease_counts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='is_cancer', y='count', data=disease_counts)\n",
    "\n",
    "plt.title('#rows associated with single diseases')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/disease_count_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['odds_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio = df_final['odds_ratio'].dropna()\n",
    "sns.boxplot(odds_ratio[odds_ratio < odds_ratio.quantile(.75)], orient='v')\n",
    "\n",
    "plt.title('Odds ratios (< 75% quantile) for all diseases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/oddsratio_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_type_counts = (df_final[['snpId', 'variant_type']]\n",
    "                       .drop_duplicates()['variant_type']\n",
    "                       .value_counts()\n",
    "                       .rename('count')\n",
    "                       .reset_index()\n",
    "                       .rename(columns={'index': 'variant_type'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.barplot(\n",
    "    x='count', y='variant_type',\n",
    "    data=variant_type_counts, orient='h', color=sns.color_palette()[0])\n",
    "\n",
    "plt.title('#variant_type in database')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/variant_type_counts.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame({\n",
    "    'diseaseId': df_final['diseaseId'],\n",
    "    'associated_genes': df_final['associated_genes'].str.split(','),\n",
    "    'gene_count': df_final['associated_genes'].str.split(',').apply(lambda x: 0 if x is None else len([e for e in x if len(e)>0]))\n",
    "})\n",
    "df_tmp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df_tmp.groupby('diseaseId')['gene_count'].sum())\n",
    "\n",
    "plt.xlabel('All diseases')\n",
    "plt.ylabel('#associated genes')\n",
    "\n",
    "unique_genes = set(g for gs in df_tmp['associated_genes'] if gs is not None for g in gs)\n",
    "plt.title(f'{len(unique_genes)} unique genes in total')\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/gene_counts.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
