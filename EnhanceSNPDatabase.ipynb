{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import tempfile\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pybiomart\n",
    "from gene_map import GeneMapper\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import load_config, split_df_row\n",
    "from tad_helper_functions import parse_tad_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm_orig\n",
    "tqdm_orig.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "cache_dir = config['output_dirs']['cache']\n",
    "images_dir = config['output_dirs']['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet = pd.read_table(\n",
    "    config['input_files']['raw_disgenet'],\n",
    "    usecols=['snpId','diseaseId','diseaseName','source'])\n",
    "df_disgenet['diseaseIdType'] = 'UMLS_CUI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "tad_data_fname = f'{results_dir}/tads_hg38.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate latest GWAS-catalog version\n",
    "\n",
    "Column definitions: https://www.ebi.ac.uk/gwas/docs/methods/curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gwascat = pd.read_table(config['input_files']['raw_gwascatalog'], low_memory=False)\n",
    "\n",
    "#df_gwascat = df_gwascat[['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT']]\n",
    "df_gwascat.dropna(subset=['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT'], inplace=True)\n",
    "df_gwascat.rename(columns={\n",
    "    'SNP_ID_CURRENT': 'snpId', 'MAPPED_TRAIT_URI': 'diseaseId', 'MAPPED_TRAIT': 'diseaseName'\n",
    "}, inplace=True)\n",
    "\n",
    "df_gwascat['snpId'] = df_gwascat['snpId'].apply(lambda x: f'rs{x}')\n",
    "df_gwascat['source'] = 'GWASCUSTOM'\n",
    "df_gwascat = split_df_row(df_gwascat, 'diseaseId', ',')\n",
    "df_gwascat['diseaseId'] = df_gwascat['diseaseId'].apply(lambda x: x.split('/')[-1])\n",
    "df_gwascat['diseaseIdType'] = df_gwascat['diseaseId'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# convert BETA to odds ratio\n",
    "df_gwascat['odds_ratio'] = df_gwascat['OR or BETA'].apply(lambda x: np.exp(x) if x < 1 else x)\n",
    "if config['filters']['OR_threshold'] is not None:\n",
    "    df_gwascat = df_gwascat[df_gwascat['odds_ratio'] > config['filters']['OR_threshold']]\n",
    "\n",
    "df_gwascat.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_gwascat])  #df_disgenet, \n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer associated gene(s)\n",
    "\n",
    "Possible columns:\n",
    "* REPORTED GENE(S): gene reported by author\n",
    "* MAPPED GENE: Gene(s) mapped to the strongest SNP (if SNP is intergenic uses upstream and downstream genes)\n",
    "* SNP_GENE_IDS: Entrez Gene ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['REPORTED GENE(S)', 'MAPPED_GENE', 'SNP_GENE_IDS']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_source = config['parameters']['associated_gene_source']\n",
    "\n",
    "if gene_source == 'reported':\n",
    "    # are gene names, must be mapped to ENTREZ\n",
    "    raw_genes = df['REPORTED GENE(S)'].str.split(', ').tolist()\n",
    "    \n",
    "    gene_blacklist = {'intergenic', 'NR'}\n",
    "    cur_genes = [g for gs in raw_genes if not isinstance(gs, float) for g in gs if g not in gene_blacklist]  # isinstance(gs,float) -> gs==np.nan\n",
    "    \n",
    "    gm = GeneMapper()\n",
    "    df_map = gm.query(id_list=cur_genes, source_id_type='Gene_Name', target_id_type='GeneID')\n",
    "    name2id = df_map.set_index('ID_from').to_dict()['ID_to']\n",
    "    \n",
    "    entrez_genes = [None \n",
    "                    if isinstance(gs, float) \n",
    "                    else [name2id[g] for g in gs if g in name2id]\n",
    "                    for gs in raw_genes]\n",
    "elif gene_source == 'mapped':\n",
    "    # are already ENTREZ IDs\n",
    "    raw_genes = df['SNP_GENE_IDS'].str.split(', ').tolist()\n",
    "    entrez_genes = [None if isinstance(gs, float) else gs for gs in raw_genes]\n",
    "else:\n",
    "    raise RuntimeError(f'Invalid gene source: \"{gene_source}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['associated_genes'] = [None if gs is None else ','.join(gs) for gs in entrez_genes]\n",
    "df[['REPORTED GENE(S)', 'MAPPED_GENE', 'SNP_GENE_IDS', 'associated_genes']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Ontology OWLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_owl_file(soup, relevant_terms):\n",
    "    \"\"\" Extract requested terms from OWL-file\n",
    "    \"\"\"\n",
    "    node_owl_data = {}\n",
    "    for entry in tqdm(soup.find_all('Class')):\n",
    "        doid = entry['rdf:about'].split('/')[-1]\n",
    "\n",
    "        # get label\n",
    "        lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "        # get requested terms\n",
    "        term_map = {term: [] for term in relevant_terms}\n",
    "        for xref in entry.find_all('oboInOwl:hasDbXref'):\n",
    "            txt = xref.get_text()\n",
    "            for term in relevant_terms:\n",
    "                if txt.startswith(f'{term}:'):\n",
    "                    idx = txt.split(':')[-1]\n",
    "                    term_map[term].append(idx)\n",
    "\n",
    "        assert doid not in node_owl_data, doid\n",
    "        node_owl_data[doid] = {\n",
    "            'label': lbl,\n",
    "            'terms': term_map\n",
    "        }\n",
    "        \n",
    "    return node_owl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['input_files']['disease_ontology']) as fd:\n",
    "    soup_doid = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_owl_data_doid = parse_owl_file(soup_doid, ['UMLS_CUI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save EFO disease labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['input_files']['exp_factor_ontology']) as fd:\n",
    "    soup = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_lbl_map = {}\n",
    "for entry in tqdm(soup.find_all('Class')):\n",
    "    if not entry.has_attr('rdf:about'):\n",
    "        continue\n",
    "    \n",
    "    efo = entry['rdf:about'].split('/')[-1]\n",
    "    lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "    assert efo not in efo_lbl_map\n",
    "    efo_lbl_map[efo] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "\n",
    "df_efolabels = pd.DataFrame(list(efo_lbl_map.items()), columns=['EFO', 'label'])\n",
    "df_efolabels.to_csv(f'{results_dir}/disease_efolabels.csv', index=False)\n",
    "\n",
    "print(df_efolabels.shape)\n",
    "df_efolabels.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease ontology as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontology_network(fname_in):\n",
    "    type_ = os.path.basename(fname_in).split('.')[0]\n",
    "    #assert type_ in ('efo', 'doid'), f'Invalid type: {type_}'\n",
    "    \n",
    "    fname_out = f'{cache_dir}/{type_}_graph.edgelist.gz'\n",
    "    if not os.path.exists(fname_out):\n",
    "        import onto2nx  # https://github.com/cthoyt/onto2nx\n",
    "        nx.write_edgelist(onto2nx.parse_owl_rdf(fname_in), fname_out)\n",
    "    else:\n",
    "        print('Cached', fname_out)\n",
    "        \n",
    "    graph = nx.read_edgelist(fname_out, create_using=nx.DiGraph()).reverse()\n",
    "    graph.name = type_\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_graph = load_ontology_network(config['input_files']['disease_ontology'])\n",
    "print(nx.info(doid_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_graph = load_ontology_network(config['input_files']['exp_factor_ontology'])\n",
    "print(nx.info(efo_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map UMLS_CUI to DOID node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_umls_map = {}\n",
    "for node, data in tqdm(node_owl_data_doid.items()):\n",
    "    assert set(data['terms'].keys()) == set(['UMLS_CUI'])\n",
    "    doid_umls_map[node] = data['terms']['UMLS_CUI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find cancer subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_diseases = set(df.diseaseId.unique())\n",
    "\n",
    "# gather all possible disease-nodes\n",
    "all_nodes_umls = [umls for doid in doid_graph.nodes() for umls in doid_umls_map[doid]]\n",
    "all_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000408')) + ['EFO_0000408']  # disease subtree (vs traits, ...)\n",
    "\n",
    "all_nodes = set(all_nodes_efo + all_nodes_umls) & given_diseases\n",
    "\n",
    "# extract all cancer diseases\n",
    "cancer_nodes_doid = (list(nx.descendants(doid_graph, 'DOID_162')) + ['DOID_162'])  # cancer subtree\n",
    "cancer_nodes_umls = [umls for doid in cancer_nodes_doid for umls in doid_umls_map[doid]]\n",
    "\n",
    "cancer_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000311')) + ['EFO_0000311']  # cancer subtree\n",
    "\n",
    "cancer_nodes = set(cancer_nodes_efo + cancer_nodes_umls) & given_diseases\n",
    "assert cancer_nodes <= all_nodes\n",
    "print(f'#various nodes: {len(cancer_nodes)}/{len(all_nodes)}/{len(given_diseases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do cancer-classification\n",
    "data_cancer = []\n",
    "for disease in tqdm(df['diseaseId'].unique()):\n",
    "    if disease in all_nodes:\n",
    "        data_cancer.append((disease, disease in cancer_nodes))\n",
    "    \n",
    "df_iscancer = pd.DataFrame(data_cancer, columns=['diseaseId','is_cancer'])\n",
    "df_iscancer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_iscancer.to_csv(f'{results_dir}/disease_cancer_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve VEP annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant consequence ontology: http://www.sequenceontology.org/browser/current_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = df['snpId'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_version = config['parameters']['source_genomiccoordinates_version']\n",
    "\n",
    "if coord_version == 'hg38':\n",
    "    mart_host = 'www.ensembl.org'\n",
    "elif coord_version == 'hg19':\n",
    "    mart_host = 'grch37.ensembl.org'\n",
    "else:\n",
    "    raise RuntimeError(f'Invalid coordinate version: \"{coord_version}\"')\n",
    "    \n",
    "print(f'Using {mart_host}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fname = f'{cache_dir}/snp_annotations.csv'\n",
    "\n",
    "if os.path.exists(fname):\n",
    "    print('Cached', fname)\n",
    "    df_anno = pd.read_csv(fname)\n",
    "else:\n",
    "    # retrieve SNP annotations\n",
    "    df_anno = None\n",
    "    while df_anno is None:\n",
    "        try:\n",
    "            server = pybiomart.Server(host=mart_host)\n",
    "            dataset = server.marts['ENSEMBL_MART_SNP'].datasets['hsapiens_snp']\n",
    "\n",
    "            df_anno = dataset.query(\n",
    "                attributes=['refsnp_id', 'chr_name', 'chrom_start', 'consequence_type_tv'],\n",
    "                filters={'snp_filter': snps},\n",
    "                use_attr_names=True)\n",
    "        except requests.HTTPError:\n",
    "            # retry if network error occurred\n",
    "            print('Next try...')\n",
    "            time.sleep(10)\n",
    "\n",
    "    # only keep \"best\" annotation\n",
    "    df_anno.drop_duplicates(subset='refsnp_id', keep='first', inplace=True)\n",
    "    \n",
    "    # mark empty consequence as 'intergenic'\n",
    "    df_anno.loc[df_anno['consequence_type_tv'].isna(), 'consequence_type_tv'] = 'intergenic_variant'\n",
    "    \n",
    "    # set column names\n",
    "    df_anno.rename(\n",
    "        columns={\n",
    "            'refsnp_id': 'snpId', 'consequence_type_tv': 'variant_type',\n",
    "            'chr_name': 'chromosome', 'chrom_start': 'position'\n",
    "        }, inplace=True)\n",
    "    \n",
    "    # save result\n",
    "    df_anno.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#SNPs in database:', df['snpId'].unique().size, f'({len(snps)})')\n",
    "print('#annotated SNPs:', df_anno['snpId'].size)\n",
    "print('#intersection:', len(set(df['snpId'].tolist()) & set(df_anno['snpId'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_anno['snpId'].tolist()) <= set(snps)\n",
    "not_annotated_snps = set(snps) - set(df_anno['snpId'].tolist())\n",
    "print(len(not_annotated_snps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer TAD relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SNP positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snppos = df_anno[['snpId', 'chromosome', 'position']].copy()\n",
    "df_snppos.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads = pd.read_table(tad_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAD statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_lengths = df_tads['tad_stop'] - df_tads['tad_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_, max_ = tad_lengths.min(), tad_lengths.max()\n",
    "min_ = min_ if min_ > 0 else 1\n",
    "if min_ == max_:\n",
    "    min_ -= 10\n",
    "    max_ += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10**np.linspace(np.log10(min_), np.log10(max_), 50)\n",
    "sns.distplot(tad_lengths, kde=False, bins=bins)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('TAD length')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(images_dir, 'tad_length_histogram.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_range_dict(row, dict_):\n",
    "    range_dict_ = dict_.get(str(row['chromosome']), None)\n",
    "    if range_dict_ is None:\n",
    "        return 'undef'\n",
    "    \n",
    "    try:\n",
    "        return range_dict_[row['position']]\n",
    "    except KeyError:\n",
    "        return 'outside'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_anno_20in = parse_tad_annotations('20in', fname=tad_data_fname)\n",
    "df_snppos['TAD_20in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20in), axis=1)\n",
    "\n",
    "tad_anno_40in = parse_tad_annotations('40in', fname=tad_data_fname)\n",
    "df_snppos['TAD_40in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40in), axis=1)\n",
    "\n",
    "tad_anno_20out = parse_tad_annotations('20out', fname=tad_data_fname)\n",
    "df_snppos['TAD_20out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20out), axis=1)\n",
    "\n",
    "tad_anno_40out = parse_tad_annotations('40out', fname=tad_data_fname)\n",
    "df_snppos['TAD_40out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40out), axis=1)\n",
    "\n",
    "tad_anno_20inout = parse_tad_annotations('20inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_20inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20inout), axis=1)\n",
    "\n",
    "tad_anno_40inout = parse_tad_annotations('40inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_40inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40inout), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snptads = df_snppos.drop(['chromosome', 'position'], axis=1)\n",
    "df_snptads.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible cell values:\n",
    "* `tad`: SNP is in TAD body (i.e. not in boundary)\n",
    "* `boundary`: SNP is in TAD boundary\n",
    "* `undef`: chromosome that SNP is in has no TAD information available\n",
    "* `outside`: SNP is outside of TAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge into DisGeNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual databases\n",
    "print('DisGeNET only:', df_disgenet.shape)\n",
    "print('Most recent GWAS-catalog: ', df_gwascat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial aggregation\n",
    "df_final = df.copy()\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer-classification\n",
    "df_final = df_final.merge(df_iscancer, on='diseaseId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAD localization\n",
    "df_final = df_final.merge(df_snptads, on='snpId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNP annotation\n",
    "df_final = df_final.merge(df_anno, how='left')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset SNP-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_type_counts = (df_final[['snpId', 'variant_type']]\n",
    "                       .drop_duplicates()['variant_type']\n",
    "                       .value_counts()\n",
    "                       .rename('count')\n",
    "                       .reset_index()\n",
    "                       .rename(columns={'index': 'variant_type'}))\n",
    "variant_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parse ontology (http://www.sequenceontology.org/browser/obob.cgi) properly\n",
    "exon_type = [\n",
    "    'missense_variant', 'non_coding_transcript_exon_variant',\n",
    "    '3_prime_UTR_variant', 'synonymous_variant', '5_prime_UTR_variant'\n",
    "]\n",
    "intron_type = ['intron_variant']\n",
    "intergenic_variant = ['intergenic_variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove exonic SNPs\n",
    "if config['filters']['variant_type'] == 'nonexonic':\n",
    "    df_final = df_final[~df_final['variant_type'].isin(exon_type)]\n",
    "\n",
    "# keep only intergenic SNPs\n",
    "if config['filters']['variant_type'] == 'intergenic':\n",
    "    df_final = df_final[df_final['variant_type'].isin(intergenic_variant)]\n",
    "\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_final.to_csv(f'{results_dir}/snpdb_enhanced.tsv', sep='\\t', index=False)\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot database statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of entries per disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_counts = (df_final['diseaseId']\n",
    "                  .value_counts()\n",
    "                  .rename('count')\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'index': 'diseaseId'})\n",
    "                  .sort_values('count')\n",
    "                  .merge(df_iscancer, on='diseaseId'))\n",
    "\n",
    "disease_counts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='is_cancer', y='count', data=disease_counts)\n",
    "\n",
    "plt.title('#rows associated with single diseases')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/disease_count_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['odds_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio = df_final['odds_ratio'].dropna()\n",
    "sns.boxplot(odds_ratio[odds_ratio < odds_ratio.quantile(.75)], orient='v')\n",
    "\n",
    "plt.title('Odds ratios (< 75% quantile) for all diseases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/oddsratio_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.barplot(\n",
    "    x='count', y='variant_type',\n",
    "    data=variant_type_counts, orient='h', color=sns.color_palette()[0])\n",
    "\n",
    "plt.title('#variant_type in database')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/variant_type_counts.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame({\n",
    "    'diseaseId': df_final['diseaseId'],\n",
    "    'associated_genes': df_final['associated_genes'].str.split(','),\n",
    "    'gene_count': df_final['associated_genes'].str.split(',').apply(lambda x: 0 if x is None else len([e for e in x if len(e)>0]))\n",
    "})\n",
    "df_tmp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df_tmp.groupby('diseaseId')['gene_count'].sum())\n",
    "\n",
    "plt.xlabel('All diseases')\n",
    "plt.ylabel('#associated genes')\n",
    "\n",
    "unique_genes = set(g for gs in df_tmp['associated_genes'] if gs is not None for g in gs)\n",
    "plt.title(f'{len(unique_genes)} unique genes in total')\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/gene_counts.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
