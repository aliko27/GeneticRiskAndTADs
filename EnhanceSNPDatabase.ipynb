{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import urllib\n",
    "import tempfile\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import load_config, split_df_row\n",
    "from tad_helper_functions import parse_tad_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm_orig\n",
    "tqdm_orig.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "cache_dir = config['output_dirs']['cache']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet = pd.read_table(\n",
    "    config['input_files']['raw_disgenet'],\n",
    "    usecols=['snpId','diseaseId','diseaseName','source'])\n",
    "df_disgenet['diseaseIdType'] = 'UMLS_CUI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "tad_data_fname = f'{results_dir}/tads_hg38.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate latest GWAS-catalog version\n",
    "\n",
    "Column definitions: https://www.ebi.ac.uk/gwas/docs/methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gwascat = pd.read_table(config['input_files']['raw_gwascatalog'], low_memory=False)\n",
    "\n",
    "#df_gwascat = df_gwascat[['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT']]\n",
    "df_gwascat.dropna(subset=['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT'], inplace=True)\n",
    "df_gwascat.rename(columns={\n",
    "    'SNP_ID_CURRENT': 'snpId', 'MAPPED_TRAIT_URI': 'diseaseId', 'MAPPED_TRAIT': 'diseaseName'\n",
    "}, inplace=True)\n",
    "\n",
    "df_gwascat['snpId'] = df_gwascat['snpId'].apply(lambda x: f'rs{x}')\n",
    "df_gwascat['source'] = 'GWASCUSTOM'\n",
    "df_gwascat = split_df_row(df_gwascat, 'diseaseId', ',')\n",
    "df_gwascat['diseaseId'] = df_gwascat['diseaseId'].apply(lambda x: x.split('/')[-1])\n",
    "df_gwascat['diseaseIdType'] = df_gwascat['diseaseId'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# convert BETA to odds ratio\n",
    "df_gwascat['odds_ratio'] = df_gwascat['OR or BETA'].apply(lambda x: np.exp(x) if x < 1 else x)\n",
    "if config['filters']['OR_threshold'] is not None:\n",
    "    df_gwascat = df_gwascat[df_gwascat['odds_ratio'] > config['filters']['OR_threshold']]\n",
    "\n",
    "df_gwascat.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_gwascat])  #df_disgenet, \n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer associated gene(s)\n",
    "\n",
    "Possible columns:\n",
    "* REPORTED GENE(S): gene reported by author\n",
    "* MAPPED GENE: Gene(s) mapped to the strongest SNP (if SNP is intergenic uses upstream and downstream genes)\n",
    "* SNP_GENE_IDS: Entrez Gene ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['REPORTED GENE(S)', 'MAPPED_GENE', 'SNP_GENE_IDS']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['associated_genes'] = df['SNP_GENE_IDS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Ontology OWLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_owl_file(soup, relevant_terms):\n",
    "    \"\"\" Extract requested terms from OWL-file\n",
    "    \"\"\"\n",
    "    node_owl_data = {}\n",
    "    for entry in tqdm(soup.find_all('Class')):\n",
    "        doid = entry['rdf:about'].split('/')[-1]\n",
    "\n",
    "        # get label\n",
    "        lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "        # get requested terms\n",
    "        term_map = {term: [] for term in relevant_terms}\n",
    "        for xref in entry.find_all('oboInOwl:hasDbXref'):\n",
    "            txt = xref.get_text()\n",
    "            for term in relevant_terms:\n",
    "                if txt.startswith(f'{term}:'):\n",
    "                    idx = txt.split(':')[-1]\n",
    "                    term_map[term].append(idx)\n",
    "\n",
    "        assert doid not in node_owl_data, doid\n",
    "        node_owl_data[doid] = {\n",
    "            'label': lbl,\n",
    "            'terms': term_map\n",
    "        }\n",
    "        \n",
    "    return node_owl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/doid.owl') as fd:\n",
    "    soup_doid = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_owl_data_doid = parse_owl_file(soup_doid, ['UMLS_CUI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save EFO disease labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/efo.owl') as fd:\n",
    "    soup = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_lbl_map = {}\n",
    "for entry in tqdm(soup.find_all('Class')):\n",
    "    if not entry.has_attr('rdf:about'):\n",
    "        continue\n",
    "    \n",
    "    efo = entry['rdf:about'].split('/')[-1]\n",
    "    lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "    assert efo not in efo_lbl_map\n",
    "    efo_lbl_map[efo] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "\n",
    "df_efolabels = pd.DataFrame(list(efo_lbl_map.items()), columns=['EFO', 'label'])\n",
    "df_efolabels.to_csv(f'{results_dir}/disease_efolabels.csv', index=False)\n",
    "\n",
    "print(df_efolabels.shape)\n",
    "df_efolabels.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease ontology as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontology_network(type_):\n",
    "    assert type_ in ('efo', 'doid'), f'Invalid type: {type_}'\n",
    "    \n",
    "    fname = f'{cache_dir}/{type_}_graph.edgelist.gz'\n",
    "    if not os.path.exists(fname):\n",
    "        import onto2nx  # https://github.com/cthoyt/onto2nx\n",
    "        nx.write_edgelist(onto2nx.parse_owl_rdf(f'data/{type_}.owl'), fname)\n",
    "    else:\n",
    "        print('Cached', fname)\n",
    "        \n",
    "    graph = nx.read_edgelist(fname, create_using=nx.DiGraph()).reverse()\n",
    "    graph.name = type_\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_graph = load_ontology_network('doid')\n",
    "print(nx.info(doid_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_graph = load_ontology_network('efo')\n",
    "print(nx.info(efo_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map UMLS_CUI to DOID node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_umls_map = {}\n",
    "for node, data in tqdm(node_owl_data_doid.items()):\n",
    "    assert set(data['terms'].keys()) == set(['UMLS_CUI'])\n",
    "    doid_umls_map[node] = data['terms']['UMLS_CUI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find cancer subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_diseases = set(df.diseaseId.unique())\n",
    "\n",
    "# gather all possible disease-nodes\n",
    "all_nodes_umls = [umls for doid in doid_graph.nodes() for umls in doid_umls_map[doid]]\n",
    "all_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000408')) + ['EFO_0000408']  # disease subtree (vs traits, ...)\n",
    "\n",
    "all_nodes = set(all_nodes_efo + all_nodes_umls) & given_diseases\n",
    "\n",
    "# extract all cancer diseases\n",
    "cancer_nodes_doid = (list(nx.descendants(doid_graph, 'DOID_162')) + ['DOID_162'])  # cancer subtree\n",
    "cancer_nodes_umls = [umls for doid in cancer_nodes_doid for umls in doid_umls_map[doid]]\n",
    "\n",
    "cancer_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000311')) + ['EFO_0000311']  # cancer subtree\n",
    "\n",
    "cancer_nodes = set(cancer_nodes_efo + cancer_nodes_umls) & given_diseases\n",
    "assert cancer_nodes <= all_nodes\n",
    "print(f'#various nodes: {len(cancer_nodes)}/{len(all_nodes)}/{len(given_diseases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do cancer-classification\n",
    "data_cancer = []\n",
    "for disease in tqdm(df['diseaseId'].unique()):\n",
    "    if disease in all_nodes:\n",
    "        data_cancer.append((disease, disease in cancer_nodes))\n",
    "    \n",
    "df_iscancer = pd.DataFrame(data_cancer, columns=['diseaseId','is_cancer'])\n",
    "df_iscancer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_iscancer.to_csv(f'{results_dir}/disease_cancer_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve VEP annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_annotations(snps):\n",
    "    _url = 'http://rest.ensembl.org/vep/human/id'\n",
    "    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n",
    "\n",
    "    r = requests.post(_url, headers=headers, data=json.dumps({'ids': snps}))\n",
    "    return r.json() if r.ok else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: in case of update, cache-file must be deleted manually\n",
    "fname = f'{cache_dir}/snp_annotations.json'\n",
    "\n",
    "if os.path.exists(f'{fname}.gz'):\n",
    "    print('Cached', f'{fname}.gz')\n",
    "    with gzip.open(f'{fname}.gz') as fd:\n",
    "        snp_anno_data = json.load(fd)\n",
    "else:\n",
    "    # setup\n",
    "    snp_anno_data = []\n",
    "\n",
    "    batch_size = 200\n",
    "    snp_list = df['snpId'].unique().tolist()\n",
    "\n",
    "    # request annotations\n",
    "    pbar = tqdm(total=len(snp_list))\n",
    "    \n",
    "    prev_i = 0\n",
    "    for i in range(batch_size, len(snp_list)+batch_size, batch_size):\n",
    "        i = min(i, len(snp_list))\n",
    "        cur_snps = snp_list[prev_i:i]\n",
    "        assert len(cur_snps) == (i-prev_i), (prev_i, i, len(cur_snps))\n",
    "\n",
    "        res = request_annotations(cur_snps)\n",
    "        assert res is not None\n",
    "        snp_anno_data.extend(res)\n",
    "\n",
    "        prev_i = i\n",
    "        pbar.update(batch_size)\n",
    "        \n",
    "    # cache results\n",
    "    with open(fname, 'w') as fd:\n",
    "        json.dump(snp_anno_data, fd)\n",
    "    !gzip $fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_anno_extract = []\n",
    "for e in snp_anno_data:\n",
    "    snp_anno_extract.append((\n",
    "        e['id'], e['most_severe_consequence'],\n",
    "        e['seq_region_name'], e['start']\n",
    "    ))\n",
    "    \n",
    "df_anno = pd.DataFrame(snp_anno_extract, columns=['snpId', 'variant_type', 'chromosome', 'position'])\n",
    "df_anno.drop_duplicates('snpId', inplace=True)\n",
    "df_anno.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#SNPs in database:', df.snpId.unique().size)\n",
    "print('#annotated SNPs:', df_anno.snpId.size)\n",
    "print('#intersection:', len(set(df.snpId.tolist()) & set(df.snpId.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer TAD relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SNP positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snppos = df_anno[['snpId', 'chromosome', 'position']].copy()\n",
    "df_snppos.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads = pd.read_table(tad_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_range_dict(row, dict_):\n",
    "    range_dict_ = dict_.get(row['chromosome'], None)\n",
    "    if range_dict_ is None:\n",
    "        return 'undef'\n",
    "    \n",
    "    return range_dict_[row['position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_anno_20in = parse_tad_annotations('20in', fname=tad_data_fname)\n",
    "df_snppos['TAD_20in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20in), axis=1)\n",
    "\n",
    "tad_anno_40in = parse_tad_annotations('40in', fname=tad_data_fname)\n",
    "df_snppos['TAD_40in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40in), axis=1)\n",
    "\n",
    "tad_anno_20out = parse_tad_annotations('20out', fname=tad_data_fname)\n",
    "df_snppos['TAD_20out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20out), axis=1)\n",
    "\n",
    "tad_anno_40out = parse_tad_annotations('40out', fname=tad_data_fname)\n",
    "df_snppos['TAD_40out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40out), axis=1)\n",
    "\n",
    "tad_anno_20inout = parse_tad_annotations('20inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_20inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20inout), axis=1)\n",
    "\n",
    "tad_anno_40inout = parse_tad_annotations('40inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_40inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40inout), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snptads = df_snppos.drop(['chromosome', 'position'], axis=1)\n",
    "df_snptads.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge into DisGeNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual databases\n",
    "print('DisGeNET only:', df_disgenet.shape)\n",
    "print('Most recent GWAS-catalog: ', df_gwascat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial aggregation\n",
    "df_final = df.copy()\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer-classification\n",
    "df_final = df_final.merge(df_iscancer, on='diseaseId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAD localization\n",
    "df_final = df_final.merge(df_snptads, on='snpId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNP annotation\n",
    "df_final = df_final.merge(df_anno, how='left')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset SNP-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_type_counts = (df_final[['snpId', 'variant_type']]\n",
    "                       .drop_duplicates()['variant_type']\n",
    "                       .value_counts()\n",
    "                       .rename('count')\n",
    "                       .reset_index()\n",
    "                       .rename(columns={'index': 'variant_type'}))\n",
    "variant_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parse ontology (http://www.sequenceontology.org/browser/obob.cgi) properly\n",
    "exon_type = [\n",
    "    'missense_variant', 'non_coding_transcript_exon_variant',\n",
    "    '3_prime_UTR_variant', 'synonymous_variant', '5_prime_UTR_variant'\n",
    "]\n",
    "intron_type = ['intron_variant']\n",
    "intergenic_variant = ['intergenic_variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove exonic SNPs\n",
    "if config['filters']['variant_type'] == 'nonexonic':\n",
    "    df_final = df_final[~df_final['variant_type'].isin(exon_type)]\n",
    "\n",
    "# keep only intergenic SNPs\n",
    "if config['filters']['variant_type'] == 'intergenic':\n",
    "    df_final = df_final[df_final['variant_type'].isin(intergenic_variant)]\n",
    "\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_final.to_csv(f'{results_dir}/snpdb_enhanced.tsv', sep='\\t', index=False)\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot database statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = config['output_dirs']['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of entries per disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_counts = (df_final['diseaseId']\n",
    "                  .value_counts()\n",
    "                  .rename('count')\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'index': 'diseaseId'})\n",
    "                  .sort_values('count')\n",
    "                  .merge(df_iscancer, on='diseaseId'))\n",
    "\n",
    "disease_counts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='is_cancer', y='count', data=disease_counts)\n",
    "\n",
    "plt.title('#rows associated with single diseases')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/disease_count_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['odds_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio = df_final['odds_ratio'].dropna()\n",
    "sns.boxplot(odds_ratio[odds_ratio < odds_ratio.quantile(.75)], orient='v')\n",
    "\n",
    "plt.title('Odds ratios (< 75% quantile) for all diseases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/oddsratio_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.barplot(\n",
    "    x='count', y='variant_type',\n",
    "    data=variant_type_counts, orient='h')\n",
    "\n",
    "plt.title('#variant_type in database')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{images_dir}/variant_type_counts.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
