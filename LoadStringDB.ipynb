{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_expand(df, id_col, val_col, sep=','):\n",
    "    \"\"\" Split dataframe cell along a separator and create an individual row per entry\n",
    "    \"\"\"\n",
    "    df_tmp = df.reset_index(drop=True)\n",
    "    \n",
    "    split_expanded = df_tmp[val_col].str.split(sep, expand=True)\n",
    "    df_expanded = df_tmp.join(split_expanded)\n",
    "\n",
    "    expanded_column_names = range(len(df_expanded.columns)-2)\n",
    "    df_long = pd.melt(\n",
    "        df_expanded,\n",
    "        id_vars=id_col, value_vars=expanded_column_names,\n",
    "        value_name=val_col\n",
    "    ).dropna().copy()\n",
    "    df_long.drop('variable', axis=1, inplace=True)\n",
    "\n",
    "    return df_long\n",
    "\n",
    "split_and_expand(\n",
    "    pd.DataFrame({\n",
    "        'A': [1,2,3],\n",
    "        'B': ['foo,bar','baz','qux,fubar,hui']\n",
    "    }),\n",
    "    'A', 'B'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppi = pd.read_table('data/9606.protein.links.v10.5.txt.gz', sep=' ')\n",
    "df_map = pd.read_table(\n",
    "    'data/9606.protein.aliases.v10.5.txt.gz', skiprows=1,\n",
    "    header=None, names=['string_protein_id', 'alias', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map StringDB to Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_ens = df_map[df_map['source']=='Ensembl'].dropna()\n",
    "string2ensemble = df_map_ens.set_index('string_protein_id').to_dict()['alias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_str2ens = df_map_ens[['string_protein_id', 'alias']]\n",
    "df_map_str2ens.columns = ('stringdb', 'ensembl')\n",
    "\n",
    "df_map_str2ens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Ensembl to Uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available Ensembl prefixes\n",
    "df_map_str2ens['ensembl'].str[:4].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembl proteins\n",
    "df_map_ens_prot = df_map_str2ens[df_map_str2ens['ensembl'].str.contains('ENSP')]\n",
    "with open('results/ensembleids_prot.txt', 'w') as fd:\n",
    "    fd.write('\\n'.join(df_map_ens_prot['ensembl'].tolist()))\n",
    "print(df_map_ens_prot.shape)\n",
    "display(df_map_ens_prot.head(1))\n",
    "\n",
    "# Ensembl transcripts\n",
    "df_map_ens_trans = df_map_str2ens[df_map_str2ens['ensembl'].str.contains('ENST')]\n",
    "with open('results/ensembleids_trans.txt', 'w') as fd:\n",
    "    fd.write('\\n'.join(df_map_ens_trans['ensembl'].tolist()))\n",
    "print(df_map_ens_trans.shape)\n",
    "display(df_map_ens_trans.head(1))\n",
    "\n",
    "# split transcripts file, as uniprot only allows <2MB uploads\n",
    "!split -b 1200000 results/ensembleids_trans.txt results/ensembleids_trans_sub_\n",
    "\n",
    "# conversion with http://www.uniprot.org/uploadlists/\n",
    "# EnsembleProtein: 65554/74741 mapped\n",
    "# EnsembleTranscript: 85862/146549 mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from webquery\n",
    "df_map_uni_prot = pd.read_table('results/ensemble_uniprot_map_prot.tsv.gz')\n",
    "df_map_uni_trans_sub_aa = pd.read_table('results/ensemble_uniprot_map_trans_sub_aa.tsv.gz')\n",
    "df_map_uni_trans_sub_ab = pd.read_table('results/ensemble_uniprot_map_trans_sub_ab.tsv.gz')\n",
    "\n",
    "display(df_map_uni_prot.head(1))\n",
    "display(df_map_uni_trans_sub_aa.head(1))\n",
    "display(df_map_uni_trans_sub_ab.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_uni_prot.columns = ('ensembl', 'uniprot')\n",
    "df_map_uni_trans_sub_aa.columns = ('ensembl', 'uniprot')\n",
    "df_map_uni_trans_sub_ab.columns = ('ensembl', 'uniprot')\n",
    "\n",
    "df_map_ens2uni_pre = pd.concat([\n",
    "    df_map_uni_prot,\n",
    "    df_map_uni_trans_sub_aa, df_map_uni_trans_sub_ab\n",
    "], axis=0)\n",
    "\n",
    "# handle multiple Ensembl entries per row\n",
    "df_map_ens2uni = split_and_expand(df_map_ens2uni_pre, 'uniprot', 'ensembl')\n",
    "\n",
    "print(df_map_ens2uni.shape)\n",
    "df_map_ens2uni.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Uniprot to Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/uniprot_prot.txt', 'w') as fd:\n",
    "    fd.write('\\n'.join(df_map_uni_prot['uniprot'].tolist()))\n",
    "with open('results/uniprot_trans_sub_aa.txt', 'w') as fd:\n",
    "    fd.write('\\n'.join(df_map_uni_trans_sub_aa['uniprot'].tolist()))\n",
    "with open('results/uniprot_trans_sub_ab.txt', 'w') as fd:\n",
    "    fd.write('\\n'.join(df_map_uni_trans_sub_ab['uniprot'].tolist()))\n",
    "\n",
    "\n",
    "# conversion with http://www.uniprot.org/uploadlists/\n",
    "# EnsembleProtein: 13265/52380 mapped to 11263 Entrez\n",
    "# EnsembleTranscript: 25504/66976 mapped 23309 Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from webquery\n",
    "df_map_uni_entrez_prot = pd.read_table('results/uniprot_entrez_prot.tsv.gz')\n",
    "df_map_uni_entrez_trans = pd.concat([\n",
    "    pd.read_table('results/uniprot_entrez_trans_sub_aa.tsv.gz'),\n",
    "    pd.read_table('results/uniprot_entrez_trans_sub_ab.tsv.gz')\n",
    "], axis=0)\n",
    "\n",
    "print(df_map_uni_entrez_prot.shape)\n",
    "display(df_map_uni_entrez_prot.head(1))\n",
    "print(df_map_uni_entrez_trans.shape)\n",
    "display(df_map_uni_entrez_trans.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_uni_entrez_prot.columns = ('uniprot', 'entrez')\n",
    "df_map_uni_entrez_trans.columns = ('uniprot', 'entrez')\n",
    "\n",
    "df_map_uni2ent = pd.concat([\n",
    "    df_map_uni_entrez_prot, df_map_uni_entrez_trans\n",
    "], axis=0)\n",
    "\n",
    "print(df_map_uni2ent.shape)\n",
    "df_map_uni2ent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mapping dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = df_map_str2ens.merge(df_map_ens2uni, on='ensembl')\n",
    "df_mapping = df_mapping.merge(df_map_uni2ent, on='uniprot')\n",
    "\n",
    "# clean multi-entries\n",
    "df_mapping['ensembl'] = df_mapping['ensembl'].str.split(',')\n",
    "df_mapping['ensembl'] = df_mapping['ensembl'].apply(lambda x: x[0])\n",
    "\n",
    "# remove duplicates\n",
    "df_mapping.drop_duplicates(inplace=True)\n",
    "\n",
    "# save result\n",
    "df_mapping.to_csv('results/gene_id_mapping.tsv.gz', index=False, compression='gzip')\n",
    "print(df_mapping.shape)\n",
    "df_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_id_map = df_mapping.set_index('stringdb').to_dict()['entrez']\n",
    "list(gene_id_map.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert stringDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(stringdb_id):\n",
    "    return gene_id_map[stringdb_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_interactions = []\n",
    "for row in tqdm(df_ppi.itertuples(), total=df_ppi.shape[0]):\n",
    "    try:\n",
    "        e1 = convert(row.protein1)\n",
    "        e2 = convert(row.protein2)\n",
    "        \n",
    "        converted_interactions.append((e1, e2, row.combined_score))\n",
    "    except KeyError:\n",
    "        pass\n",
    "df_conv = pd.DataFrame(converted_interactions, columns=['protein1', 'protein2', 'combined_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv.to_csv('data/stringdb_entrez.tsv.gz', sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'StringDB shape conversion: {df_ppi.shape} -> {df_conv.shape}')\n",
    "display(df_ppi.head())\n",
    "display(df_conv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_orig = nx.convert_matrix.from_pandas_edgelist(\n",
    "    df_ppi, source='protein1', target='protein2', edge_attr='combined_score')\n",
    "graph_conv = nx.convert_matrix.from_pandas_edgelist(\n",
    "    df_conv, source='protein1', target='protein2', edge_attr='combined_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_info(name, graph):\n",
    "    print(f'--- {name} ---')\n",
    "    print(nx.info(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_info('Original graph', graph_orig)\n",
    "print()\n",
    "graph_info('Converted graph', graph_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
