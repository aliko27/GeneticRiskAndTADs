{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import urllib\n",
    "import tempfile\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils import load_config, split_df_row\n",
    "from tad_helper_functions import parse_tad_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm_orig\n",
    "tqdm_orig.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet = pd.read_table(\n",
    "    config['input_files']['raw_disgenet'],\n",
    "    usecols=['snpId','diseaseId','diseaseName','source'])\n",
    "df_disgenet['diseaseIdType'] = 'UMLS_CUI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disgenet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide whether to use hg19 or hg38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel and rerun all cells after changing this cell\n",
    "USE_HG38 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HG38:\n",
    "    genome_version = 'hg38'\n",
    "    \n",
    "    results_dir = config['output_dirs']['results']\n",
    "    tad_data_fname = f'{results_dir}/tads_hESC_hg38.tsv'\n",
    "    \n",
    "    def snp_position_convert(row):\n",
    "        # don't do anything, as positions are already in hg38\n",
    "        return row.position\n",
    "else:\n",
    "    genome_version = 'hg19'\n",
    "    tad_data_fname = config['input_files']['tad_coordinates_hg19']\n",
    "    \n",
    "    # get hg19 SNP-positions\n",
    "    df_snp_pos_map = pd.read_csv('results/snp_positions_hg19.csv')\n",
    "    df_snp_pos_map['chrom'] = df_snp_pos_map['chrom'].apply(lambda x: x[3:])\n",
    "    df_snp_pos_map['pos'] = list(zip(df_snp_pos_map['chrom'], df_snp_pos_map['end']))\n",
    "\n",
    "    snp_pos_hg19 = df_snp_pos_map.set_index('SNPS').to_dict()['pos']\n",
    "    def snp_position_convert(row):\n",
    "        # convert BP-position from hg38 to hg19\n",
    "        if row.snpId not in snp_pos_hg19:\n",
    "            return np.nan\n",
    "        \n",
    "        chrom, pos_hg19 = snp_pos_hg19[row.snpId]\n",
    "        assert row.chromosome == chrom\n",
    "        return pos_hg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate latest GWAS-catalog version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gwascat = pd.read_table(config['input_files']['raw_gwascatalog'], low_memory=False)\n",
    "\n",
    "df_gwascat = df_gwascat[['SNP_ID_CURRENT', 'MAPPED_TRAIT_URI', 'MAPPED_TRAIT']]\n",
    "df_gwascat.dropna(inplace=True)\n",
    "df_gwascat.rename(columns={\n",
    "    'SNP_ID_CURRENT': 'snpId', 'MAPPED_TRAIT_URI': 'diseaseId', 'MAPPED_TRAIT': 'diseaseName'\n",
    "}, inplace=True)\n",
    "\n",
    "df_gwascat['snpId'] = df_gwascat['snpId'].apply(lambda x: f'rs{x}')\n",
    "df_gwascat['source'] = 'GWASCUSTOM'\n",
    "df_gwascat = split_df_row(df_gwascat, 'diseaseId', ',')\n",
    "df_gwascat['diseaseId'] = df_gwascat['diseaseId'].apply(lambda x: x.split('/')[-1])\n",
    "df_gwascat['diseaseIdType'] = df_gwascat['diseaseId'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# convert BETA to odds ratio\n",
    "df_gwascat['odds_ratio'] = df_gwascat['OR or BETA'].apply(lambda x: np.exp(x) if x < 1 else x)\n",
    "#df_gwascat = df_gwascat[df_gwascat['odds_ratio'] > 2]\n",
    "\n",
    "df_gwascat.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_disgenet, df_gwascat])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Ontology OWLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_owl_file(soup, relevant_terms):\n",
    "    \"\"\" Extract requested terms from OWL-file\n",
    "    \"\"\"\n",
    "    node_owl_data = {}\n",
    "    for entry in tqdm(soup.find_all('Class')):\n",
    "        doid = entry['rdf:about'].split('/')[-1]\n",
    "\n",
    "        # get label\n",
    "        lbl = entry.find('rdfs:label').get_text()\n",
    "\n",
    "        # get requested terms\n",
    "        term_map = {term: [] for term in relevant_terms}\n",
    "        for xref in entry.find_all('oboInOwl:hasDbXref'):\n",
    "            txt = xref.get_text()\n",
    "            for term in relevant_terms:\n",
    "                if txt.startswith(f'{term}:'):\n",
    "                    idx = txt.split(':')[-1]\n",
    "                    term_map[term].append(idx)\n",
    "\n",
    "        assert doid not in node_owl_data, doid\n",
    "        node_owl_data[doid] = {\n",
    "            'label': lbl,\n",
    "            'terms': term_map\n",
    "        }\n",
    "        \n",
    "    return node_owl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/doid.owl') as fd:\n",
    "    soup_doid = BeautifulSoup(fd, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_owl_data_doid = parse_owl_file(soup_doid, ['UMLS_CUI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disease ontology as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontology_network(type_):\n",
    "    assert type_ in ('efo', 'doid'), f'Invalid type: {type_}'\n",
    "    \n",
    "    fname = f'cache/{type_}_graph.edgelist.gz'\n",
    "    if not os.path.exists(fname):\n",
    "        import onto2nx  # https://github.com/cthoyt/onto2nx\n",
    "        nx.write_edgelist(onto2nx.parse_owl_rdf(f'data/{type_}.owl'), fname)\n",
    "    else:\n",
    "        print('Cached', fname)\n",
    "        \n",
    "    graph = nx.read_edgelist(fname, create_using=nx.DiGraph()).reverse()\n",
    "    graph.name = type_\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_graph = load_ontology_network('doid')\n",
    "print(nx.info(doid_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efo_graph = load_ontology_network('efo')\n",
    "print(nx.info(efo_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map UMLS_CUI to DOID node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_umls_map = {}\n",
    "for node, data in tqdm(node_owl_data_doid.items()):\n",
    "    assert set(data['terms'].keys()) == set(['UMLS_CUI'])\n",
    "    doid_umls_map[node] = data['terms']['UMLS_CUI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find cancer subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_diseases = set(df.diseaseId.unique())\n",
    "\n",
    "# gather all possible disease-nodes\n",
    "all_nodes_umls = [umls for doid in doid_graph.nodes() for umls in doid_umls_map[doid]]\n",
    "all_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000408')) + ['EFO_0000408']  # disease subtree (vs traits, ...)\n",
    "\n",
    "all_nodes = set(all_nodes_efo + all_nodes_umls) & given_diseases\n",
    "\n",
    "# extract all cancer diseases\n",
    "cancer_nodes_doid = (list(nx.descendants(doid_graph, 'DOID_162')) + ['DOID_162'])  # cancer subtree\n",
    "cancer_nodes_umls = [umls for doid in cancer_nodes_doid for umls in doid_umls_map[doid]]\n",
    "\n",
    "cancer_nodes_efo = list(nx.descendants(efo_graph, 'EFO_0000311')) + ['EFO_0000311']  # cancer subtree\n",
    "\n",
    "cancer_nodes = set(cancer_nodes_efo + cancer_nodes_umls) & given_diseases\n",
    "assert cancer_nodes <= all_nodes\n",
    "print(f'#various nodes: {len(cancer_nodes)}/{len(all_nodes)}/{len(given_diseases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do cancer-classification\n",
    "data_cancer = []\n",
    "for disease in tqdm(df['diseaseId'].unique()):\n",
    "    if disease in all_nodes:\n",
    "        data_cancer.append((disease, disease in cancer_nodes))\n",
    "    \n",
    "df_iscancer = pd.DataFrame(data_cancer, columns=['diseaseId','is_cancer'])\n",
    "df_iscancer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_iscancer.to_csv(f'{results_dir}/disease_cancer_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNP annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve VEP annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_annotations(snps):\n",
    "    _url = 'http://rest.ensembl.org/vep/human/id'\n",
    "    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n",
    "\n",
    "    r = requests.post(_url, headers=headers, data=json.dumps({'ids': snps}))\n",
    "    return r.json() if r.ok else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: in case of update, cache-file must be deleted manually\n",
    "fname = 'cache/snp_annotations.json'\n",
    "\n",
    "if os.path.exists(f'{fname}.gz'):\n",
    "    print('Cached', f'{fname}.gz')\n",
    "    with gzip.open(f'{fname}.gz') as fd:\n",
    "        snp_anno_data = json.load(fd)\n",
    "else:\n",
    "    # setup\n",
    "    snp_anno_data = []\n",
    "\n",
    "    batch_size = 200\n",
    "    snp_list = df['snpId'].unique().tolist()\n",
    "\n",
    "    # request annotations\n",
    "    pbar = tqdm(total=len(snp_list))\n",
    "    \n",
    "    prev_i = 0\n",
    "    for i in range(batch_size, len(snp_list)+batch_size, batch_size):\n",
    "        i = min(i, len(snp_list))\n",
    "        cur_snps = snp_list[prev_i:i]\n",
    "        assert len(cur_snps) == (i-prev_i), (prev_i, i, len(cur_snps))\n",
    "\n",
    "        res = request_annotations(cur_snps)\n",
    "        assert res is not None\n",
    "        snp_anno_data.extend(res)\n",
    "\n",
    "        prev_i = i\n",
    "        pbar.update(batch_size)\n",
    "        \n",
    "    # cache results\n",
    "    with open(fname, 'w') as fd:\n",
    "        json.dump(snp_anno_data, fd)\n",
    "    !gzip $fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_anno_extract = []\n",
    "for e in snp_anno_data:\n",
    "    snp_anno_extract.append((\n",
    "        e['id'], e['most_severe_consequence'],\n",
    "        e['seq_region_name'], e['start']\n",
    "    ))\n",
    "    \n",
    "df_anno = pd.DataFrame(snp_anno_extract, columns=['snpId', 'variant_type', 'chromosome', 'position'])\n",
    "df_anno.drop_duplicates('snpId', inplace=True)\n",
    "df_anno.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#SNPs in database:', df.snpId.unique().size)\n",
    "print('#annotated SNPs:', df_anno.snpId.size)\n",
    "print('#intersection:', len(set(df.snpId.tolist()) & set(df.snpId.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer TAD relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SNP positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snppos = df_anno[['snpId', 'chromosome', 'position']].copy()\n",
    "df_snppos.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads = pd.read_table(tad_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_range_dict(row, dict_):\n",
    "    range_dict_ = dict_.get(row['chromosome'], None)\n",
    "    if range_dict_ is None:\n",
    "        return 'undef'\n",
    "    \n",
    "    return range_dict_[row['position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_anno_20in = parse_tad_annotations('20in', fname=tad_data_fname)\n",
    "df_snppos['TAD_20in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20in), axis=1)\n",
    "\n",
    "tad_anno_40in = parse_tad_annotations('40in', fname=tad_data_fname)\n",
    "df_snppos['TAD_40in'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40in), axis=1)\n",
    "\n",
    "tad_anno_20out = parse_tad_annotations('20out', fname=tad_data_fname)\n",
    "df_snppos['TAD_20out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20out), axis=1)\n",
    "\n",
    "tad_anno_40out = parse_tad_annotations('40out', fname=tad_data_fname)\n",
    "df_snppos['TAD_40out'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40out), axis=1)\n",
    "\n",
    "tad_anno_20inout = parse_tad_annotations('20inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_20inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_20inout), axis=1)\n",
    "\n",
    "tad_anno_40inout = parse_tad_annotations('40inout', fname=tad_data_fname)\n",
    "df_snppos['TAD_40inout'] = df_snppos.progress_apply(lambda x: access_range_dict(x, tad_anno_40inout), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snptads = df_snppos.drop(['chromosome', 'position'], axis=1)\n",
    "df_snptads.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge into DisGeNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual databases\n",
    "print('DisGeNET only:', df_disgenet.shape)\n",
    "print('Most recent GWAS-catalog: ', df_gwascat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial aggregation\n",
    "df_final = df.copy()\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer-classification\n",
    "df_final = df_final.merge(df_iscancer, on='diseaseId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAD localization\n",
    "df_final = df_final.merge(df_snptads, on='snpId')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNP annotation\n",
    "df_final = df_final.merge(df_anno, how='left')\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset SNP-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[['snpId', 'variant_type']].drop_duplicates().variant_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parse ontology (http://www.sequenceontology.org/browser/obob.cgi) properly\n",
    "exon_type = [\n",
    "    'missense_variant', 'non_coding_transcript_exon_variant',\n",
    "    '3_prime_UTR_variant', 'synonymous_variant', '5_prime_UTR_variant'\n",
    "]\n",
    "intron_type = ['intron_variant']\n",
    "intergenic_variant = ['intergenic_variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove exonic SNPs\n",
    "#df_final = df_final[~df_final['variant_type'].isin(exon_type)]\n",
    "\n",
    "# keep only intergenic SNPs\n",
    "#df_final = df_final[df_final['variant_type'].isin(intergenic_variant)]\n",
    "\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = config['output_dirs']['results']\n",
    "df_final.to_csv(f'{results_dir}/disgenet_enhanced_{genome_version}.tsv', sep='\\t', index=False)\n",
    "df_final.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
